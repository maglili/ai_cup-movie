{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load basic library\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load torch library\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# custum module\n",
    "from tools import *\n",
    "\n",
    "# keep reandom seed\n",
    "seed_val = 0\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "\n",
    "# check gpu\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "# load data\n",
    "with open(\"./data/pkl/X_train_list.pkl\", \"rb\") as f:\n",
    "    X_train_list = pickle.load(f)\n",
    "with open(\"./data/pkl/X_valid_list.pkl\", \"rb\") as f:\n",
    "    X_valid_list = pickle.load(f)\n",
    "with open(\"./data/pkl/y_train_list.pkl\", \"rb\") as f:\n",
    "    y_train_list = pickle.load(f)\n",
    "with open(\"./data/pkl/y_valid_list.pkl\", \"rb\") as f:\n",
    "    y_valid_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing for bert input\n",
      "tokenizing for bert input\n",
      "5\n",
      "torch.Size([18777, 400])\n",
      "torch.Size([18777, 400])\n",
      "torch.Size([18777])\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=False)\n",
    "\n",
    "(\n",
    "    input_ids_train_dict,\n",
    "    attention_masks_train_dict,\n",
    "    labels_train_dict,\n",
    ") = tokenizing_for_bert(\n",
    "    X_train_list, y_train_list, tokenizer\n",
    ")  # max_len = 400\n",
    "\n",
    "input_ids_cv_dict, attention_masks_cv_dict, labels_cv_dict = tokenizing_for_bert(\n",
    "    X_valid_list, y_valid_list, tokenizer, train=False\n",
    ")\n",
    "\n",
    "print(len(input_ids_train_dict))\n",
    "print(input_ids_train_dict[\"tr_1\"].shape)\n",
    "print(attention_masks_train_dict[\"tr_1\"].shape)\n",
    "print(labels_train_dict[\"tr_1\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-29403cf677e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23472, 400])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_data = torch.cat((input_ids_train_dict[\"tr_1\"], input_ids_cv_dict[\"va_1\"]), 0)\n",
    "# all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# token_length = []\n",
    "# for i in all_data:\n",
    "#     token_length.append(sum(i!=0).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.33452624403546\n",
      "400\n",
      "4\n",
      "127.15932762513401\n"
     ]
    }
   ],
   "source": [
    "# print(np.mean(token_length))\n",
    "# print(np.max(token_length))\n",
    "# print(np.min(token_length))\n",
    "# print(np.std(token_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.8667348329925\n",
      "512\n",
      "4\n",
      "153.99549015975495\n"
     ]
    }
   ],
   "source": [
    "# print(np.mean(token_length))\n",
    "# print(np.max(token_length))\n",
    "# print(np.min(token_length))\n",
    "# print(np.std(token_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.TensorDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Prepare torch dataset\n",
    "tr_set = []\n",
    "va_set = []\n",
    "for idx in range(len(input_ids_train_dict)):\n",
    "    tr_set.append(\n",
    "        TensorDataset(\n",
    "            input_ids_train_dict[\"tr_\" + str(idx)],\n",
    "            attention_masks_train_dict[\"tr_\" + str(idx)],\n",
    "            labels_train_dict[\"tr_\" + str(idx)],\n",
    "        )\n",
    "    )\n",
    "    va_set.append(\n",
    "        TensorDataset(\n",
    "            input_ids_cv_dict[\"va_\" + str(idx)],\n",
    "            attention_masks_cv_dict[\"va_\" + str(idx)],\n",
    "            labels_cv_dict[\"va_\" + str(idx)],\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(type(tr_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 4\n",
      "batch_size: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hypterparameter\n",
    "epochs = 4\n",
    "batch_size = 8\n",
    "print(\"epochs:\", epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-cased\",\n",
    "#     num_labels=2,\n",
    "#     output_attentions=False,\n",
    "#     output_hidden_states=False,\n",
    "#     hidden_dropout_prob=0.5,\n",
    "#     attention_probs_dropout_prob=0.5,\n",
    "# )\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Don't apply weight decay to any parameters whose names include these tokens.\n",
    "# # (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# # Separate the `weight` parameters from the `bias` parameters.\n",
    "# # - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01.\n",
    "# # - For the `bias` parameters, the 'weight_decay_rate' is 0.0.\n",
    "# optimizer_grouped_parameters = [\n",
    "#     # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#      'weight_decay_rate': 0.1},\n",
    "\n",
    "#     # Filter for parameters which *do* include those.\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#      'weight_decay_rate': 0.0}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold]: 0\n",
      "Num of train samples: 18777\n",
      "Num of valid samples: 4695\n",
      "\n",
      "training loss: 0.47\n",
      "training acc: 0.79\n",
      "-------------------------\n",
      "train loss: 0.37\n",
      "train acc: 0.89\n",
      "-------------------------\n",
      "valid loss: 0.42\n",
      "valid acc: 0.88\n",
      "-------------------------\n",
      "=========================\n",
      "training loss: 0.38\n",
      "training acc: 0.88\n",
      "-------------------------\n",
      "train loss: 0.27\n",
      "train acc: 0.93\n",
      "-------------------------\n",
      "valid loss: 0.36\n",
      "valid acc: 0.90\n",
      "-------------------------\n",
      "=========================\n",
      "training loss: 0.33\n",
      "training acc: 0.91\n",
      "-------------------------\n",
      "train loss: 0.25\n",
      "train acc: 0.94\n",
      "-------------------------\n",
      "valid loss: 0.40\n",
      "valid acc: 0.91\n",
      "-------------------------\n",
      "=========================\n",
      "training loss: 0.28\n",
      "training acc: 0.93\n",
      "-------------------------\n",
      "train loss: 0.24\n",
      "train acc: 0.95\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [1:05:45<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.41\n",
      "valid acc: 0.91\n",
      "-------------------------\n",
      "=========================\n",
      "*************************\n",
      "*************************\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "training_hist = []\n",
    "\n",
    "for fold in tqdm(range(len(tr_set))):\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        hidden_dropout_prob=0.4,\n",
    "        attention_probs_dropout_prob=0.25,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # This code is taken from:\n",
    "    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
    "\n",
    "    # Don't apply weight decay to any parameters whose names include these tokens.\n",
    "    # (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "    # Separate the `weight` parameters from the `bias` parameters.\n",
    "    # - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01.\n",
    "    # - For the `bias` parameters, the 'weight_decay_rate' is 0.0.\n",
    "    optimizer_grouped_parameters = [\n",
    "        # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay_rate\": 0.1,\n",
    "        },\n",
    "        # Filter for parameters which *do* include those.\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay_rate\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Note - `optimizer_grouped_parameters` only includes the parameter values, not\n",
    "    # the names.\n",
    "\n",
    "    N_train = len(tr_set[fold])\n",
    "    N_test = len(va_set[fold])\n",
    "    print(\"\\n[Fold]:\", fold)\n",
    "    print(\"Num of train samples:\", N_train)\n",
    "    print(\"Num of valid samples:\", N_test)\n",
    "    print()\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=4e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(tr_set[fold], shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        va_set[fold], shuffle=False, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs].\n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=total_steps * 0.1, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_dataloader,\n",
    "        valid_loader=validation_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        N_train=N_train,\n",
    "        N_test=N_test,\n",
    "        device=device,\n",
    "        scheduler=scheduler,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    training_hist.append(history)\n",
    "    print(\"*\" * 25)\n",
    "    print(\"*\" * 25)\n",
    "    print(\"*\" * 25)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def final_metric(history, metric_path, mtype=\"train\"):\n",
    "    \"\"\"\n",
    "    Calculate metric.\n",
    "    \"\"\"\n",
    "    # init\n",
    "    ACC = []\n",
    "    LOSS = []\n",
    "    RECALL = []\n",
    "    SPECIFICITY = []\n",
    "    PRECISION = []\n",
    "    NPV = []\n",
    "    F1 = []\n",
    "    MCC = []\n",
    "    AUC = []\n",
    "    FPR = []\n",
    "    TPR = []\n",
    "\n",
    "    for i in range(len(history)):\n",
    "\n",
    "        (TP, FP, TN, FN) = history[i][mtype + \"_metric\"][-1]\n",
    "        auc = history[i][mtype + \"_auc\"][-1]\n",
    "        fpr = history[i][mtype + \"_fpr\"][-1]\n",
    "        tpr = history[i][mtype + \"_tpr\"][-1]\n",
    "        loss = history[i][mtype + \"_loss\"][-1]\n",
    "\n",
    "        acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "        recall = TP / (TP + FN) if TP != 0 else 0  # 召回率是在所有正樣本當中，能夠預測多少正樣本的比例\n",
    "        specificity = TN / (TN + FP) if TN != 0 else 0  # 特異度是在所有負樣本當中，能夠預測多少負樣本的比例\n",
    "        precision = TP / (TP + FP) if TP != 0 else 0  # 準確率為在所有預測為正樣本中，有多少為正樣本\n",
    "        npv = TN / (TN + FN) if TN != 0 else 0  # npv為在所有預測為負樣本中，有多少為負樣本\n",
    "        f1 = (\n",
    "            (2 * recall * precision) / (recall + precision)\n",
    "            if (recall + precision) != 0\n",
    "            else 0\n",
    "        )  # F1-score則是兩者的調和平均數\n",
    "\n",
    "        mcc = (\n",
    "            (TP * TN - FP * FN)\n",
    "            / np.sqrt(((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n",
    "            if ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) != 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        ACC.append(acc)\n",
    "        LOSS.append(loss)\n",
    "        RECALL.append(recall)\n",
    "        SPECIFICITY.append(specificity)\n",
    "        PRECISION.append(precision)\n",
    "        NPV.append(npv)\n",
    "        F1.append(f1)\n",
    "        MCC.append(mcc)\n",
    "        AUC.append(auc)\n",
    "        FPR.append(fpr)\n",
    "        TPR.append(tpr)\n",
    "\n",
    "    print(\"\\n[\" + mtype + \" average]\\n\")\n",
    "    print(\"ACC: {:.2}\".format((np.mean(ACC))))\n",
    "    print(\"LOSS: {:.2}\".format(np.mean(LOSS)))\n",
    "    print()\n",
    "    print(\"Recall: {:.2}\".format(np.mean(RECALL)))\n",
    "    print(\"Specificity: {:.2}\".format(np.mean(SPECIFICITY)))\n",
    "    print(\"Precision: {:.2}\".format(np.mean(PRECISION)))\n",
    "    print(\"NPV: {:.2}\".format(np.mean(NPV)))\n",
    "    print()\n",
    "    print(\"F1: {:.2}\".format(np.mean(F1)))\n",
    "    print(\"MCC: {:.2}\".format(np.mean(MCC)))\n",
    "    print(\"AUC: {:.2}\".format(np.mean(AUC)))\n",
    "    print()\n",
    "\n",
    "    # save result\n",
    "    save_metrics(\n",
    "        metric_path,\n",
    "        mtype,\n",
    "        ACC,\n",
    "        LOSS,\n",
    "        RECALL,\n",
    "        SPECIFICITY,\n",
    "        PRECISION,\n",
    "        NPV,\n",
    "        F1,\n",
    "        MCC,\n",
    "        AUC,\n",
    "    )\n",
    "\n",
    "\n",
    "def save_metrics(\n",
    "    metric_path,\n",
    "    mtype,\n",
    "    ACC,\n",
    "    LOSS,\n",
    "    RECALL,\n",
    "    SPECIFICITY,\n",
    "    PRECISION,\n",
    "    NPV,\n",
    "    F1,\n",
    "    MCC,\n",
    "    AUC,\n",
    "):\n",
    "    \"\"\"\n",
    "    save metrics as csv files\n",
    "    \"\"\"\n",
    "    filename = mtype + \"_metrics.csv\"\n",
    "    save_path = os.path.join(metric_path, filename)\n",
    "    with open(save_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\"\\t\")\n",
    "        writer.writerow([\"\\n[\" + mtype + \" average]\\n\"])\n",
    "        writer.writerow([\"ACC: {:.2}\".format((np.mean(ACC)))])\n",
    "        writer.writerow([\"LOSS: {:.2}\".format(np.mean(LOSS))])\n",
    "        writer.writerow([\"Recall: {:.2}\".format(np.mean(RECALL))])\n",
    "        writer.writerow([\"Specificity: {:.2}\".format(np.mean(SPECIFICITY))])\n",
    "        writer.writerow([\"Precision: {:.2}\".format(np.mean(PRECISION))])\n",
    "        writer.writerow([\"NPV: {:.2}\".format(np.mean(NPV))])\n",
    "        writer.writerow([\"F1: {:.2}\".format(np.mean(F1))])\n",
    "        writer.writerow([\"MCC: {:.2}\".format(np.mean(MCC))])\n",
    "        writer.writerow([\"AUC: {:.2}\".format(np.mean(AUC))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save trainin_history\n",
    "with open(os.path.join(history_path, \"/hist.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(training_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train average]\n",
      "\n",
      "ACC: 0.95\n",
      "LOSS: 0.24\n",
      "\n",
      "Recall: 0.96\n",
      "Specificity: 0.94\n",
      "Precision: 0.94\n",
      "NPV: 0.96\n",
      "\n",
      "F1: 0.95\n",
      "MCC: 0.9\n",
      "AUC: 0.99\n",
      "\n",
      "\n",
      "[valid average]\n",
      "\n",
      "ACC: 0.91\n",
      "LOSS: 0.41\n",
      "\n",
      "Recall: 0.93\n",
      "Specificity: 0.89\n",
      "Precision: 0.9\n",
      "NPV: 0.93\n",
      "\n",
      "F1: 0.92\n",
      "MCC: 0.83\n",
      "AUC: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_metric(training_hist, metric_path=metric_path, mtype=\"train\")\n",
    "final_metric(training_hist, metric_path=metric_path, mtype=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(model_path, \"model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lichang/projects/ai_cup-movie/result/bert-base-cased_bs_8_epo4/metrics'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calc_avg(training_hist):\n",
    "    \"\"\"\n",
    "    Plot learning curve\n",
    "    \"\"\"\n",
    "\n",
    "    a1 = a2 = a3 = a4 = []  # init\n",
    "\n",
    "    for i in range(len(training_hist)):\n",
    "        if i == 0:\n",
    "            a1 = np.array(training_hist[0][\"train_loss\"].copy())\n",
    "            a2 = np.array(training_hist[0][\"valid_loss\"].copy())\n",
    "            a3 = np.array(training_hist[0][\"train_acc\"].copy())\n",
    "            a4 = np.array(training_hist[0][\"valid_acc\"].copy())\n",
    "            continue\n",
    "        a1 = a1 + np.array(training_hist[i][\"train_loss\"])\n",
    "        a2 = a2 + np.array(training_hist[i][\"valid_loss\"])\n",
    "        a3 = a3 + np.array(training_hist[i][\"train_acc\"])\n",
    "        a4 = a4 + np.array(training_hist[i][\"valid_acc\"])\n",
    "\n",
    "    a1 /= len(training_hist)\n",
    "    a2 /= len(training_hist)\n",
    "    a3 /= len(training_hist)\n",
    "    a4 /= len(training_hist)\n",
    "\n",
    "    a1 = a1.tolist()\n",
    "    a2 = a2.tolist()\n",
    "    a3 = a3.tolist()\n",
    "    a4 = a4.tolist()\n",
    "\n",
    "    return a1, a2, a3, a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(training_hist, fig_path, mtype=\"train\"):\n",
    "    \"\"\"\n",
    "    plot roc curve and save as png\n",
    "    \"\"\"\n",
    "    for i in range(len(training_hist)):\n",
    "        auc = training_hist[i][mtype + \"_auc\"][-1]\n",
    "        fpr = training_hist[i][mtype + \"_fpr\"][-1]\n",
    "        tpr = training_hist[i][mtype + \"_tpr\"][-1]\n",
    "\n",
    "        plt.plot(fpr, tpr, label=\"Fold-\" + str(i) + \" AUC = %0.2f\" % auc)\n",
    "\n",
    "    plt.title(mtype + \" roc curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.plot([0, 1], [0, 1], \"r--\")\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    filename = mtype + \"-roc.png\"\n",
    "    plt.savefig(os.path.join(fig_path, filename))  # , bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_figure(training_hist, fig_path):\n",
    "\n",
    "    a1, a2, a3, a4 = calc_avg(training_hist)\n",
    "\n",
    "    # color\n",
    "    tr_color = [\"#2ff5f2\", \"#2ff5e8\", \"#2ff5c0\", \"#2fbdf5\", \"#2f99f5\"]\n",
    "    val_color = [\"#f5952f\", \"#f5ac2f\", \"#f5c02f\", \"#f5d72f\", \"#f5ee2f\"]\n",
    "\n",
    "    # train loss\n",
    "    for idx in range(len(training_hist)):\n",
    "        plt.plot(\n",
    "            training_hist[idx][\"train_loss\"],\n",
    "            \"--\",\n",
    "            alpha=0.6,\n",
    "            label=\"train\" + str(idx),\n",
    "            # color=color,\n",
    "        )\n",
    "    plt.plot(a1, label=\"average training\")\n",
    "\n",
    "    # valid loss\n",
    "    for idx in range(len(training_hist)):\n",
    "        plt.plot(\n",
    "            training_hist[idx][\"valid_loss\"],\n",
    "            \"--\",\n",
    "            alpha=0.6,\n",
    "            label=\"valid\" + str(idx),\n",
    "            # color=color,\n",
    "        )\n",
    "\n",
    "    plt.plot(a2, label=\"average valid\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.title(\"training / valid loss vs iterations\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(fig_path, \"loss.png\"))  # , bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # train acc\n",
    "    for idx in range(len(training_hist)):\n",
    "        plt.plot(\n",
    "            training_hist[idx][\"train_acc\"],\n",
    "            \"--\",\n",
    "            alpha=0.6,\n",
    "            label=\"train\" + str(idx),\n",
    "            # color=color,\n",
    "        )\n",
    "    plt.plot(a3, label=\"average training\")\n",
    "\n",
    "    # valid acc\n",
    "    for idx in range(len(training_hist)):\n",
    "        plt.plot(\n",
    "            training_hist[idx][\"valid_acc\"],\n",
    "            \"--\",\n",
    "            alpha=0.6,\n",
    "            label=\"valid\" + str(idx),\n",
    "            # color=color,\n",
    "        )\n",
    "    plt.plot(a4, label=\"average valid\")\n",
    "    plt.ylabel(\"acc\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.5, 1])\n",
    "    plt.legend()\n",
    "    plt.title(\"training / valid acc vs iterations\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(fig_path, \"acc.png\"))  # , bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # roc\n",
    "    plot_roc(training_hist, fig_path, mtype=\"train\")\n",
    "    plot_roc(training_hist, fig_path, mtype=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_figure(training_hist, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"./data/pkl/X_test.pkl\", \"rb\") as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open(\"./data/pkl/y_test.pkl\", \"rb\") as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_path(model_name, batch_size, epochs, mode=\"train\"):\n",
    "    # setting path\n",
    "    cwd = os.getcwd()\n",
    "    print(\"cwd:\", cwd)\n",
    "\n",
    "    folder_name = model_name + \"_bs_\" + str(batch_size) + \"_epo\" + str(epochs)\n",
    "\n",
    "    metric_path = os.path.abspath(\n",
    "        os.path.join(cwd, \"result\", folder_name, mode, \"metrics\")\n",
    "    )\n",
    "    model_path = os.path.abspath(\n",
    "        os.path.join(cwd, \"result\", folder_name, mode, \"model\")\n",
    "    )\n",
    "    history_path = os.path.abspath(\n",
    "        os.path.join(cwd, \"result\", folder_name, mode, \"history\")\n",
    "    )\n",
    "    fig_path = os.path.abspath(\n",
    "        os.path.join(cwd, \"result\", folder_name, mode, \"figures\")\n",
    "    )\n",
    "\n",
    "    print(\"metric_path:\", metric_path)\n",
    "    print(\"model_path:\", model_path)\n",
    "    print(\"history_path:\", history_path)\n",
    "    print(\"fig_path:\", fig_path)\n",
    "\n",
    "    if not os.path.isdir(metric_path):\n",
    "        os.makedirs(metric_path)\n",
    "    if not os.path.isdir(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    if not os.path.isdir(history_path):\n",
    "        os.makedirs(history_path)\n",
    "    if not os.path.isdir(fig_path):\n",
    "        os.makedirs(fig_path)\n",
    "\n",
    "    return metric_path, model_path, history_path, fig_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20892    A student filmmaker enlists a B-grade actress ...\n",
       "13280    This movie has a \"big production\" feel that I ...\n",
       "29002    A vampire's's henchman wants to call her after...\n",
       "6858     Don't get me wrong, I assumed this movie would...\n",
       "21664    Swedish action movies have over the past few y...\n",
       "                               ...                        \n",
       "12939    \"Three Daring Daughters\" is a sickly sweet, ro...\n",
       "20460    I too am a House Party Fan...House Party I is ...\n",
       "9273     I just came back from a pre-release viewing of...\n",
       "6213     This is a very intriguing short movie by David...\n",
       "29034    Yes, that's right, it is. I firmly believe tha...\n",
       "Name: review, Length: 5869, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20892    0\n",
       "13280    1\n",
       "29002    0\n",
       "6858     0\n",
       "21664    0\n",
       "        ..\n",
       "12939    0\n",
       "20460    0\n",
       "9273     1\n",
       "6213     1\n",
       "29034    1\n",
       "Name: sentiment, Length: 5869, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd /home/lichang/projects/ai_cup-movie\n",
      "metric_path: /home/lichang/projects/ai_cup-movie/result/bert-base-cased_bs_8_epo4/test/metrics\n",
      "model_path: /home/lichang/projects/ai_cup-movie/result/bert-base-cased_bs_8_epo4/test/model\n",
      "history_path: /home/lichang/projects/ai_cup-movie/result/bert-base-cased_bs_8_epo4/test/history\n",
      "fig_path: /home/lichang/projects/ai_cup-movie/result/bert-base-cased_bs_8_epo4/test/figures\n"
     ]
    }
   ],
   "source": [
    "metric_path, _, history_path, fig_path = setting_path(\n",
    "    model_name, batch_size, epochs, mode=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_for_bert_eval(sent_list, sent_label, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize abstracts and return data as Bert model input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"tokenizing for bert input\")\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    # For every sentence...\n",
    "    for sent in sent_list:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=400,  # 512,  # Pad & truncate all sentences.\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids_list.append(encoded_dict[\"input_ids\"])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_list.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids_list, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks_list, dim=0)\n",
    "    labels = torch.tensor(sent_label)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing for bert input\n"
     ]
    }
   ],
   "source": [
    "input_ids_te, attention_masks_te, labels_te = tokenizing_for_bert_eval(\n",
    "    X_test.values, y_test.values, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = TensorDataset(input_ids_te, attention_masks_te, labels_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5869"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    hidden_dropout_prob=0.4,\n",
    "    attention_probs_dropout_prob=0.25,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "PATH = \"./result/bert-base-cased_bs_8_epo4/model/model.pkl\"\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data(\n",
    "    model,\n",
    "    test_loader,\n",
    "    N_test,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    eval the data.\n",
    "    \"\"\"\n",
    "\n",
    "    useful_stuff = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"test_auc\": [],\n",
    "        \"test_metric\": [],\n",
    "        \"test_fpr\": [],\n",
    "        \"test_tpr\": [],\n",
    "    }\n",
    "\n",
    "    # evaluate test metrics\n",
    "    (\n",
    "        model,\n",
    "        correct,\n",
    "        training_loss,\n",
    "        (TP, FP, TN, FN),\n",
    "        y_list,\n",
    "        yhat_list,\n",
    "    ) = batch_iter(model, test_loader, None, None, device, training=False)\n",
    "\n",
    "    useful_stuff = calc_metrics(\n",
    "        N_test,\n",
    "        test_loader,\n",
    "        correct,\n",
    "        training_loss,\n",
    "        (TP, FP, TN, FN),\n",
    "        y_list,\n",
    "        yhat_list,\n",
    "        useful_stuff,\n",
    "        type=\"test\",\n",
    "    )\n",
    "\n",
    "    return useful_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.43\n",
      "test acc: 0.91\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "training_hist = []\n",
    "\n",
    "N_test = len(testdataset)\n",
    "\n",
    "test_loader = DataLoader(testdataset, shuffle=False, batch_size=16)\n",
    "\n",
    "history = eval_data(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    N_test=N_test,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "training_hist.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lichang/projects/ai_cup-movie/result/bert-base-cased_bs_8_epo4/test/history'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trainin_history\n",
    "with open(os.path.join(history_path, \"hist.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(training_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[test average]\n",
      "\n",
      "ACC: 0.91\n",
      "LOSS: 0.43\n",
      "\n",
      "Recall: 0.93\n",
      "Specificity: 0.9\n",
      "Precision: 0.9\n",
      "NPV: 0.92\n",
      "\n",
      "F1: 0.92\n",
      "MCC: 0.83\n",
      "AUC: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_metric(training_hist, metric_path=metric_path, mtype=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/pkl/test_new.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>Robert Lansing plays a scientist experimenting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>Well I've enjoy this movie, even though someti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>First things first - though I believe Joel Sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>I watched this movie on the grounds that Amber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>A certain sexiness underlines even the dullest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>30370</td>\n",
       "      <td>It is difficult to rate a writer/director's fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>18654</td>\n",
       "      <td>After watching this movie once, it quickly bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>47985</td>\n",
       "      <td>Even though i sat and watched the whole thing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>9866</td>\n",
       "      <td>Warning Spoilers following. Superb recreation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>35559</td>\n",
       "      <td>My, my, my: Peter Cushing and Donald Pleasance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                             review\n",
       "0      22622  Robert Lansing plays a scientist experimenting...\n",
       "1      10162  Well I've enjoy this movie, even though someti...\n",
       "2      17468  First things first - though I believe Joel Sch...\n",
       "3      42579  I watched this movie on the grounds that Amber...\n",
       "4        701  A certain sexiness underlines even the dullest...\n",
       "...      ...                                                ...\n",
       "29336  30370  It is difficult to rate a writer/director's fi...\n",
       "29337  18654  After watching this movie once, it quickly bec...\n",
       "29338  47985  Even though i sat and watched the whole thing,...\n",
       "29339   9866  Warning Spoilers following. Superb recreation ...\n",
       "29340  35559  My, my, my: Peter Cushing and Donald Pleasance...\n",
       "\n",
       "[29341 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test[\"review\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_for_bert_pred(sent_list, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize abstracts and return data as Bert model input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"tokenizing for bert input\")\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    # For every sentence...\n",
    "    for sent in sent_list:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=400,  # 512,  # Pad & truncate all sentences.\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids_list.append(encoded_dict[\"input_ids\"])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_list.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids_list, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks_list, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing for bert input\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks = tokenizing_for_bert_pred(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = TensorDataset(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29341"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    hidden_dropout_prob=0.4,\n",
    "    attention_probs_dropout_prob=0.25,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "PATH = \"./result/bert-base-cased_bs_8_epo4/train/model/model.pkl\"\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_data(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    prediction the data.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            output = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = output[0]\n",
    "            logits = output[1]\n",
    "\n",
    "            yhat_prob = F.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            training_loss += loss.item()\n",
    "            _, yhat = torch.max(logits.data, 1)\n",
    "            correct += (yhat == b_labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testdataset, shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "\n",
    "        output = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = output[0]\n",
    "\n",
    "        _, yhat = torch.max(logits.data, 1)\n",
    "\n",
    "        pred.extend(yhat.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29341\n"
     ]
    }
   ],
   "source": [
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29341"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>30370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29337</th>\n",
       "      <td>18654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>47985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>9866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>35559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  sentiment\n",
       "0      22622          1\n",
       "1      10162          1\n",
       "2      17468          0\n",
       "3      42579          0\n",
       "4        701          0\n",
       "...      ...        ...\n",
       "29336  30370          0\n",
       "29337  18654          1\n",
       "29338  47985          0\n",
       "29339   9866          0\n",
       "29340  35559          0\n",
       "\n",
       "[29341 rows x 2 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "submission = pd.DataFrame({\"ID\": test[\"ID\"].values, \"sentiment\": pred})\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
